{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from senteval import engine\n",
    "\n",
    "from utils import *\n",
    "from bert import *\n",
    "\n",
    "model_name, maxlen = 'prajjwal1/bert-tiny', 128\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to Encode Sentences into Embeddings using BERT\n",
    "def encode(sentences):\n",
    "    #print(sentences)\n",
    "    encoded_input = tokenizer(sentences, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n",
    "    sentence_ids = encoded_input['input_ids']\n",
    "    sentence_mask = encoded_input['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = model(input_ids=sentence_ids, attention_mask=sentence_mask)[1]\n",
    "\n",
    "    return features.numpy()\n",
    "\n",
    "# Set Up SentEval Parameters\n",
    "params_senteval = {\n",
    "    'task_path': 'senteval/data',\n",
    "    'usepytorch': True,\n",
    "    'kfold': 5\n",
    "}\n",
    "params_senteval['classifier'] = {'nhid': 0, 'optim': 'adam', 'batch_size': 128,\n",
    "                                 'tenacity': 5, 'epoch_size': 10}\n",
    "\n",
    "# Define the SentEval Preparation and Evaluation functions\n",
    "def prepare(params, samples):\n",
    "    return None\n",
    "\n",
    "def batcher(params, batch):\n",
    "    sentences = [' '.join(s) for s in batch]\n",
    "    embeddings = encode(sentences)\n",
    "    return embeddings\n",
    "\n",
    "# Run the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sentence Similarity\\n')\n",
    "transfer_tasks = ['STSBenchmark', 'STS16', 'STS15', 'STS14', 'STS13', 'STS12', 'SICKRelatedness', 'SICKEntailment']\n",
    "\n",
    "print('Classifier\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/classifier_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results1 = se.eval(transfer_tasks)\n",
    "print(results1)\n",
    "with open('results_classifier.txt', 'w') as f:\n",
    "    f.write(str(results1))\n",
    "\n",
    "print('Triplet\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/triplet_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results2 = se.eval(transfer_tasks)\n",
    "print(results2)\n",
    "with open('results_triplet.txt', 'w') as f:\n",
    "    f.write(str(results2))\n",
    "\n",
    "print('Warping\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/warping_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results3 = se.eval(transfer_tasks)\n",
    "print(results3)\n",
    "with open('results_warping.txt', 'w') as f:\n",
    "    f.write(str(results3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sentiment Analysis\\n')\n",
    "transfer_tasks = ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'SST5', 'TREC', 'MRPC']\n",
    "\n",
    "print('Classifier\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/classifier_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results1 = se.eval(transfer_tasks)\n",
    "print(results1)\n",
    "with open('sentiment_results_classifier.txt', 'w') as f:\n",
    "    f.write(str(results1))\n",
    "\n",
    "print('Triplet\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/triplet_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results2 = se.eval(transfer_tasks)\n",
    "print(results2)\n",
    "with open('sentiment_results_triplet.txt', 'w') as f:\n",
    "    f.write(str(results2))\n",
    "\n",
    "print('Warping\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/warping_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results3 = se.eval(transfer_tasks)\n",
    "print(results3)\n",
    "with open('sentiment_results_warping.txt', 'w') as f:\n",
    "    f.write(str(results3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probing\\n')\n",
    "transfer_tasks = ['Length', 'WordContent', 'Depth', 'TopConstituents',\n",
    "                'BigramShift', 'Tense', 'SubjNumber', 'ObjNumber',\n",
    "                'OddManOut', 'CoordinationInversion']\n",
    "\n",
    "print('Classifier\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/classifier_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results1 = se.eval(transfer_tasks)\n",
    "print(results1)\n",
    "with open('probing_results_classifier.txt', 'w') as f:\n",
    "    f.write(str(results1))\n",
    "\n",
    "print('Triplet\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/triplet_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results2 = se.eval(transfer_tasks)\n",
    "print(results2)\n",
    "with open('probing_results_triplet.txt', 'w') as f:\n",
    "    f.write(str(results2))\n",
    "\n",
    "print('Warping\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/warping_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results3 = se.eval(transfer_tasks)\n",
    "print(results3)\n",
    "with open('probing_results_warping.txt', 'w') as f:\n",
    "    f.write(str(results3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Others\\n')\n",
    "transfer_tasks = ['coco-caption', 'sts17', 'sts18', 'sts19']\n",
    "\n",
    "print('Classifier\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/classifier_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results1 = se.eval(transfer_tasks)\n",
    "print(results1)\n",
    "with open('others_results_classifier.txt', 'w') as f:\n",
    "    f.write(str(results1))\n",
    "\n",
    "print('Triplet\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/triplet_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results2 = se.eval(transfer_tasks)\n",
    "print(results2)\n",
    "with open('others_results_triplet.txt', 'w') as f:\n",
    "    f.write(str(results2))\n",
    "\n",
    "print('Warping\\n')\n",
    "model = Bert({'hidden_size': 128, 'num_attention_heads': 2, 'num_hidden_layers': 2, \n",
    "            'intermediate_size': 512, 'vocab_size': 30522}\n",
    "            ).load_model('weights/warping_bert.pth')\n",
    "se = engine.SE(params_senteval, batcher, prepare)\n",
    "results3 = se.eval(transfer_tasks)\n",
    "print(results3)\n",
    "with open('others_results_warping.txt', 'w') as f:\n",
    "    f.write(str(results3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
