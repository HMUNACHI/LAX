{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVPkyE6r8cc9"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M4UUlYyX8eh7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from bert import *\n",
        "from utils import *\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "model_name, maxlen = 'prajjwal1/bert-tiny', 128\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "sts_dataset = load_sts_dataset('data/stsbenchmark.tsv.gz')\n",
        "tokenized_test = tokenize_sentence_pair_dataset(sts_dataset['test'], tokenizer)\n",
        "sts_testloader = get_dataloader(tokenized_test, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8dEdZoc815Y"
      },
      "source": [
        "# Evaluate baseline TinyBERT model on STS benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters: 4385920\n",
            "\n",
            "Pearson correlation: 0.43\n",
            "Spearman correlation: 0.42\n"
          ]
        }
      ],
      "source": [
        "bert_config = {\"hidden_size\": 128, \"num_attention_heads\": 2, \"num_hidden_layers\": 2, \"intermediate_size\": 512, \"vocab_size\": 30522}\n",
        "bert_path = 'weights/bert_tiny.bin'\n",
        "bert = Bert(bert_config).load_model(bert_path)\n",
        "print('Parameters:', sum(p.numel() for p in bert.parameters() if p.requires_grad))\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "results_from_pretrained = eval_loop(bert, sts_testloader, device)\n",
        "torch.save(bert.state_dict(), 'weights/baseline_bert.pth')\n",
        "print(f'\\nPearson correlation: {results_from_pretrained[0]:.2f}\\nSpearman correlation: {results_from_pretrained[1]:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train TinyBERT with NLI Classifier and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 645798\n",
            "Validation: 13209\n",
            "4386946\n",
            "\n",
            "Pearson correlation: 0.47\n",
            "Spearman correlation: 0.47\n",
            "Epoch [1/30] - Train Loss: 0.5929, Validation Loss: 0.5566\n",
            "\n",
            "Pearson correlation: 0.50\n",
            "Spearman correlation: 0.50\n",
            "Epoch [2/30] - Train Loss: 0.5384, Validation Loss: 0.5289\n",
            "\n",
            "Pearson correlation: 0.51\n",
            "Spearman correlation: 0.52\n",
            "Epoch [3/30] - Train Loss: 0.5153, Validation Loss: 0.5156\n",
            "\n",
            "Pearson correlation: 0.53\n",
            "Spearman correlation: 0.55\n",
            "Epoch [4/30] - Train Loss: 0.5005, Validation Loss: 0.5076\n",
            "\n",
            "Pearson correlation: 0.54\n",
            "Spearman correlation: 0.55\n",
            "Epoch [5/30] - Train Loss: 0.4886, Validation Loss: 0.5012\n",
            "\n",
            "Pearson correlation: 0.53\n",
            "Spearman correlation: 0.55\n",
            "Epoch [6/30] - Train Loss: 0.4790, Validation Loss: 0.5006\n",
            "\n",
            "Pearson correlation: 0.54\n",
            "Spearman correlation: 0.56\n",
            "Epoch [7/30] - Train Loss: 0.4706, Validation Loss: 0.4982\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.56\n",
            "Epoch [8/30] - Train Loss: 0.4636, Validation Loss: 0.4949\n",
            "\n",
            "Pearson correlation: 0.54\n",
            "Spearman correlation: 0.56\n",
            "Epoch [9/30] - Train Loss: 0.4569, Validation Loss: 0.4941\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [10/30] - Train Loss: 0.4509, Validation Loss: 0.4948\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [11/30] - Train Loss: 0.4455, Validation Loss: 0.4947\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.56\n",
            "Epoch [12/30] - Train Loss: 0.4406, Validation Loss: 0.4940\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.56\n",
            "Epoch [13/30] - Train Loss: 0.4361, Validation Loss: 0.4939\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.56\n",
            "Epoch [14/30] - Train Loss: 0.4317, Validation Loss: 0.4935\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.56\n",
            "Epoch [15/30] - Train Loss: 0.4279, Validation Loss: 0.4949\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.56\n",
            "Epoch [16/30] - Train Loss: 0.4241, Validation Loss: 0.4948\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [17/30] - Train Loss: 0.4208, Validation Loss: 0.4923\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [18/30] - Train Loss: 0.4171, Validation Loss: 0.4955\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [19/30] - Train Loss: 0.4145, Validation Loss: 0.4961\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.56\n",
            "Epoch [20/30] - Train Loss: 0.4116, Validation Loss: 0.4946\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [21/30] - Train Loss: 0.4091, Validation Loss: 0.4953\n",
            "\n",
            "Pearson correlation: 0.56\n",
            "Spearman correlation: 0.58\n",
            "Epoch [22/30] - Train Loss: 0.4068, Validation Loss: 0.4944\n",
            "\n",
            "Pearson correlation: 0.54\n",
            "Spearman correlation: 0.57\n",
            "Epoch [23/30] - Train Loss: 0.4043, Validation Loss: 0.4933\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [24/30] - Train Loss: 0.4020, Validation Loss: 0.4949\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [25/30] - Train Loss: 0.4000, Validation Loss: 0.4969\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.58\n",
            "Epoch [26/30] - Train Loss: 0.3978, Validation Loss: 0.4959\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [27/30] - Train Loss: 0.3962, Validation Loss: 0.4968\n",
            "\n",
            "Pearson correlation: 0.54\n",
            "Spearman correlation: 0.57\n",
            "Epoch [28/30] - Train Loss: 0.3947, Validation Loss: 0.4953\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [29/30] - Train Loss: 0.3930, Validation Loss: 0.4968\n",
            "\n",
            "Pearson correlation: 0.55\n",
            "Spearman correlation: 0.57\n",
            "Epoch [30/30] - Train Loss: 0.3913, Validation Loss: 0.4970\n"
          ]
        }
      ],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert_config,\n",
        "                 bert_path: str):\n",
        "        \"\"\"\n",
        "        Initialize a BERT-based classifier model.\n",
        "\n",
        "        Args:\n",
        "            bert_config: BERT model configuration.\n",
        "            bert_path (str): Path to a pre-trained BERT model checkpoint.\n",
        "\n",
        "        Attributes:\n",
        "            bert (Bert): Pre-trained BERT model.\n",
        "\n",
        "        \"\"\"\n",
        "        super(Classifier, self).__init__()\n",
        "        self.bert = Bert(bert_config).load_model(bert_path)\n",
        "        self.classification_head = nn.Linear(128*4,2)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self,\n",
        "                sentence_1_ids: torch.Tensor,\n",
        "                sentence_1_mask: torch.Tensor,\n",
        "                sentence_2_ids: torch.Tensor,\n",
        "                sentence_2_mask: torch.Tensor,\n",
        "                labels: torch.Tensor) -> torch.Tensor:\n",
        "        \n",
        "        embeddings_1 = self.bert(input_ids=sentence_1_ids, attention_mask=sentence_1_mask)[1]\n",
        "        embeddings_2 = self.bert(input_ids=sentence_2_ids, attention_mask=sentence_2_mask)[1]\n",
        "        return self.loss(embeddings_1, embeddings_2, labels)\n",
        "\n",
        "    def loss(self, a: torch.Tensor, b: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "        logits = self.classification_head(torch.cat((a, b, torch.abs(a-b), a*b), dim=-1))\n",
        "        preds = nn.functional.softmax(logits, dim=-1)\n",
        "        return self.criterion(preds, labels)\n",
        "\n",
        "model = train_and_evaluate(Classifier, tokenizer, sts_testloader, num_epochs=30, batch_size=256, maxlen=maxlen)\n",
        "torch.save(model.bert.state_dict(), 'weights/classifier_bert.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train TinyBERT with Contrastive Loss on all data and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 645798\n",
            "Validation: 13209\n",
            "4385921\n",
            "\n",
            "Pearson correlation: 0.59\n",
            "Spearman correlation: 0.62\n",
            "Epoch [1/5] - Train Loss: 4.8601, Validation Loss: 4.6939\n",
            "\n",
            "Pearson correlation: 0.61\n",
            "Spearman correlation: 0.63\n",
            "Epoch [2/5] - Train Loss: 4.5413, Validation Loss: 4.5590\n",
            "\n",
            "Pearson correlation: 0.60\n",
            "Spearman correlation: 0.62\n",
            "Epoch [3/5] - Train Loss: 4.4258, Validation Loss: 4.5711\n",
            "\n",
            "Pearson correlation: 0.59\n",
            "Spearman correlation: 0.61\n",
            "Epoch [4/5] - Train Loss: 4.3490, Validation Loss: 4.5083\n",
            "\n",
            "Pearson correlation: 0.57\n",
            "Spearman correlation: 0.60\n",
            "Epoch [5/5] - Train Loss: 4.2917, Validation Loss: 4.6276\n"
          ]
        }
      ],
      "source": [
        "class NTXent(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert_config,\n",
        "                 bert_path: str):\n",
        "        \"\"\"\n",
        "        Initialize a BERT-based classifier model.\n",
        "\n",
        "        Args:\n",
        "            bert_config: BERT model configuration.\n",
        "            bert_path (str): Path to a pre-trained BERT model checkpoint.\n",
        "\n",
        "        Attributes:\n",
        "            bert (Bert): Pre-trained BERT model.\n",
        "\n",
        "        \"\"\"\n",
        "        super(NTXent, self).__init__()\n",
        "        self.bert = Bert(bert_config).load_model(bert_path)\n",
        "        self.temperature = nn.Parameter(torch.tensor([0.01]))\n",
        "\n",
        "    def forward(self,\n",
        "                sentence_1_ids: torch.Tensor,\n",
        "                sentence_1_mask: torch.Tensor,\n",
        "                sentence_2_ids: torch.Tensor,\n",
        "                sentence_2_mask: torch.Tensor,\n",
        "                labels: torch.Tensor) -> torch.Tensor:\n",
        "        \n",
        "        embeddings_1 = self.bert(input_ids=sentence_1_ids, attention_mask=sentence_1_mask)[1]\n",
        "        embeddings_2 = self.bert(input_ids=sentence_2_ids, attention_mask=sentence_2_mask)[1]\n",
        "        return self.loss(embeddings_1, embeddings_2)\n",
        "\n",
        "    def loss(self, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "        features = torch.cat([a, b], dim=0)\n",
        "        normalized_features = F.normalize(features, p=2, dim=1)\n",
        "        similarity_matrix = torch.mm(normalized_features, normalized_features.T)\n",
        "        logits = similarity_matrix / self.temperature\n",
        "        batch_size = a.size(0)\n",
        "        mask = torch.eye(batch_size, dtype=torch.bool, device=a.device)\n",
        "        labels = torch.cat([torch.arange(batch_size, device=a.device), torch.arange(batch_size, device=a.device)])\n",
        "        logits = logits.masked_select(~torch.block_diag(mask, mask)).view(2 * batch_size, -1)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        return loss\n",
        "    \n",
        "model = train_and_evaluate(NTXent, tokenizer, sts_testloader, num_epochs=5, batch_size=256, maxlen=maxlen)\n",
        "torch.save(model.bert.state_dict(), 'weights/ntxent_bert.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train TinyBERT with Triplet Loss on all data and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 645798\n",
            "Validation: 13209\n",
            "4385920\n",
            "\n",
            "Pearson correlation: 0.59\n",
            "Spearman correlation: 0.62\n",
            "Epoch [1/30] - Train Loss: 0.7692, Validation Loss: 0.5963\n",
            "\n",
            "Pearson correlation: 0.62\n",
            "Spearman correlation: 0.64\n",
            "Epoch [2/30] - Train Loss: 0.6293, Validation Loss: 0.5580\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.66\n",
            "Epoch [3/30] - Train Loss: 0.5811, Validation Loss: 0.5309\n",
            "\n",
            "Pearson correlation: 0.64\n",
            "Spearman correlation: 0.65\n",
            "Epoch [4/30] - Train Loss: 0.5500, Validation Loss: 0.5229\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.66\n",
            "Epoch [5/30] - Train Loss: 0.5258, Validation Loss: 0.5044\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.66\n",
            "Epoch [6/30] - Train Loss: 0.5078, Validation Loss: 0.5006\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.66\n",
            "Epoch [7/30] - Train Loss: 0.4908, Validation Loss: 0.4926\n",
            "\n",
            "Pearson correlation: 0.67\n",
            "Spearman correlation: 0.67\n",
            "Epoch [8/30] - Train Loss: 0.4782, Validation Loss: 0.4869\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.66\n",
            "Epoch [9/30] - Train Loss: 0.4656, Validation Loss: 0.4796\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.67\n",
            "Epoch [10/30] - Train Loss: 0.4548, Validation Loss: 0.4772\n",
            "\n",
            "Pearson correlation: 0.67\n",
            "Spearman correlation: 0.67\n",
            "Epoch [11/30] - Train Loss: 0.4458, Validation Loss: 0.4781\n",
            "\n",
            "Pearson correlation: 0.67\n",
            "Spearman correlation: 0.67\n",
            "Epoch [12/30] - Train Loss: 0.4364, Validation Loss: 0.4688\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.67\n",
            "Epoch [13/30] - Train Loss: 0.4284, Validation Loss: 0.4786\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.68\n",
            "Epoch [14/30] - Train Loss: 0.4213, Validation Loss: 0.4811\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.67\n",
            "Epoch [15/30] - Train Loss: 0.4151, Validation Loss: 0.4737\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.67\n",
            "Epoch [16/30] - Train Loss: 0.4083, Validation Loss: 0.4760\n",
            "\n",
            "Pearson correlation: 0.69\n",
            "Spearman correlation: 0.68\n",
            "Epoch [17/30] - Train Loss: 0.4026, Validation Loss: 0.4712\n",
            "\n",
            "Pearson correlation: 0.69\n",
            "Spearman correlation: 0.68\n",
            "Epoch [18/30] - Train Loss: 0.3967, Validation Loss: 0.4737\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.68\n",
            "Epoch [19/30] - Train Loss: 0.3916, Validation Loss: 0.4731\n",
            "\n",
            "Pearson correlation: 0.69\n",
            "Spearman correlation: 0.68\n",
            "Epoch [20/30] - Train Loss: 0.3863, Validation Loss: 0.4713\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.68\n",
            "Epoch [21/30] - Train Loss: 0.3824, Validation Loss: 0.4692\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.68\n",
            "Epoch [22/30] - Train Loss: 0.3778, Validation Loss: 0.4710\n",
            "\n",
            "Pearson correlation: 0.69\n",
            "Spearman correlation: 0.68\n",
            "Epoch [23/30] - Train Loss: 0.3739, Validation Loss: 0.4779\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.68\n",
            "Epoch [24/30] - Train Loss: 0.3696, Validation Loss: 0.4770\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.68\n",
            "Epoch [25/30] - Train Loss: 0.3656, Validation Loss: 0.4731\n",
            "\n",
            "Pearson correlation: 0.70\n",
            "Spearman correlation: 0.69\n",
            "Epoch [26/30] - Train Loss: 0.3625, Validation Loss: 0.4753\n",
            "\n",
            "Pearson correlation: 0.68\n",
            "Spearman correlation: 0.68\n",
            "Epoch [27/30] - Train Loss: 0.3594, Validation Loss: 0.4719\n",
            "\n",
            "Pearson correlation: 0.69\n",
            "Spearman correlation: 0.68\n",
            "Epoch [28/30] - Train Loss: 0.3559, Validation Loss: 0.4727\n",
            "\n",
            "Pearson correlation: 0.69\n",
            "Spearman correlation: 0.68\n",
            "Epoch [29/30] - Train Loss: 0.3521, Validation Loss: 0.4731\n",
            "\n",
            "Pearson correlation: 0.69\n",
            "Spearman correlation: 0.68\n",
            "Epoch [30/30] - Train Loss: 0.3492, Validation Loss: 0.4699\n"
          ]
        }
      ],
      "source": [
        "class Triplet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert_config,\n",
        "                 bert_path: str):\n",
        "        \"\"\"\n",
        "        Initialize a BERT-based classifier model.\n",
        "\n",
        "        Args:\n",
        "            bert_config: BERT model configuration.\n",
        "            bert_path (str): Path to a pre-trained BERT model checkpoint.\n",
        "\n",
        "        Attributes:\n",
        "            bert (Bert): Pre-trained BERT model.\n",
        "\n",
        "        \"\"\"\n",
        "        super(Triplet, self).__init__()\n",
        "        self.bert = Bert(bert_config).load_model(bert_path)\n",
        "        self.loss_fn = nn.TripletMarginLoss(margin=1.0)\n",
        "\n",
        "    def forward(self,\n",
        "                sentence_1_ids: torch.Tensor,\n",
        "                sentence_1_mask: torch.Tensor,\n",
        "                sentence_2_ids: torch.Tensor,\n",
        "                sentence_2_mask: torch.Tensor,\n",
        "                labels: torch.Tensor) -> torch.Tensor:\n",
        "        \n",
        "        embeddings_1 = self.bert(input_ids=sentence_1_ids, attention_mask=sentence_1_mask)[1]\n",
        "        embeddings_2 = self.bert(input_ids=sentence_2_ids, attention_mask=sentence_2_mask)[1]\n",
        "        return self.loss(embeddings_1, embeddings_2, labels)\n",
        "\n",
        "    def loss(self, anchors, positives, labels):\n",
        "        distance_matrix = torch.cdist(anchors, positives, p=2)\n",
        "        labels_expanded = labels.unsqueeze(1)\n",
        "        negative_mask = labels_expanded != labels_expanded.t()\n",
        "        max_distance = distance_matrix.max().detach()\n",
        "        distance_matrix_masked = torch.where(negative_mask, distance_matrix, max_distance)\n",
        "        hardest_negatives = positives[distance_matrix_masked.argmin(dim=1)]\n",
        "        loss = self.loss_fn(anchors, positives, hardest_negatives)\n",
        "        return loss\n",
        "\n",
        "model = train_and_evaluate(Triplet, tokenizer, sts_testloader, num_epochs=30, batch_size=256, maxlen=maxlen)\n",
        "torch.save(model.bert.state_dict(), 'weights/triplet_bert.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9__3lchp97BM"
      },
      "source": [
        "# Train TinyBERT with Warping Loss on entailment data alone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 322742\n",
            "Validation: 6452\n",
            "4517249\n",
            "\n",
            "Pearson correlation: 0.69\n",
            "Spearman correlation: 0.68\n",
            "Epoch [1/30] - Train Loss: 2.4485, Validation Loss: 1.4134\n",
            "\n",
            "Pearson correlation: 0.70\n",
            "Spearman correlation: 0.70\n",
            "Epoch [2/30] - Train Loss: 1.4441, Validation Loss: 1.1642\n",
            "\n",
            "Pearson correlation: 0.71\n",
            "Spearman correlation: 0.70\n",
            "Epoch [3/30] - Train Loss: 1.1857, Validation Loss: 1.0127\n",
            "\n",
            "Pearson correlation: 0.71\n",
            "Spearman correlation: 0.71\n",
            "Epoch [4/30] - Train Loss: 1.0374, Validation Loss: 0.9618\n",
            "\n",
            "Pearson correlation: 0.72\n",
            "Spearman correlation: 0.71\n",
            "Epoch [5/30] - Train Loss: 0.9365, Validation Loss: 0.8999\n",
            "\n",
            "Pearson correlation: 0.72\n",
            "Spearman correlation: 0.71\n",
            "Epoch [6/30] - Train Loss: 0.8625, Validation Loss: 0.8598\n",
            "\n",
            "Pearson correlation: 0.72\n",
            "Spearman correlation: 0.71\n",
            "Epoch [7/30] - Train Loss: 0.8006, Validation Loss: 0.8459\n",
            "\n",
            "Pearson correlation: 0.72\n",
            "Spearman correlation: 0.71\n",
            "Epoch [8/30] - Train Loss: 0.7528, Validation Loss: 0.8360\n",
            "\n",
            "Pearson correlation: 0.72\n",
            "Spearman correlation: 0.71\n",
            "Epoch [9/30] - Train Loss: 0.7089, Validation Loss: 0.8305\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [10/30] - Train Loss: 0.6694, Validation Loss: 0.8228\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [11/30] - Train Loss: 0.6374, Validation Loss: 0.8110\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [12/30] - Train Loss: 0.6083, Validation Loss: 0.8106\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [13/30] - Train Loss: 0.5815, Validation Loss: 0.8205\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [14/30] - Train Loss: 0.5561, Validation Loss: 0.8265\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [15/30] - Train Loss: 0.5357, Validation Loss: 0.8041\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [16/30] - Train Loss: 0.5173, Validation Loss: 0.8177\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [17/30] - Train Loss: 0.4972, Validation Loss: 0.8374\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [18/30] - Train Loss: 0.4808, Validation Loss: 0.8268\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [19/30] - Train Loss: 0.4646, Validation Loss: 0.8480\n",
            "\n",
            "Pearson correlation: 0.74\n",
            "Spearman correlation: 0.72\n",
            "Epoch [20/30] - Train Loss: 0.4485, Validation Loss: 0.8318\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [21/30] - Train Loss: 0.4360, Validation Loss: 0.8435\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [22/30] - Train Loss: 0.4238, Validation Loss: 0.8545\n",
            "\n",
            "Pearson correlation: 0.73\n",
            "Spearman correlation: 0.72\n",
            "Epoch [23/30] - Train Loss: 0.4149, Validation Loss: 0.8662\n",
            "\n",
            "Pearson correlation: 0.74\n",
            "Spearman correlation: 0.72\n",
            "Epoch [24/30] - Train Loss: 0.4012, Validation Loss: 0.8643\n",
            "\n",
            "Pearson correlation: 0.74\n",
            "Spearman correlation: 0.72\n",
            "Epoch [25/30] - Train Loss: 0.3922, Validation Loss: 0.8724\n",
            "\n",
            "Pearson correlation: 0.74\n",
            "Spearman correlation: 0.73\n",
            "Epoch [26/30] - Train Loss: 0.3833, Validation Loss: 0.8658\n",
            "\n",
            "Pearson correlation: 0.74\n",
            "Spearman correlation: 0.73\n",
            "Epoch [27/30] - Train Loss: 0.3753, Validation Loss: 0.8977\n",
            "\n",
            "Pearson correlation: 0.74\n",
            "Spearman correlation: 0.72\n",
            "Epoch [28/30] - Train Loss: 0.3655, Validation Loss: 0.8939\n",
            "\n",
            "Pearson correlation: 0.74\n",
            "Spearman correlation: 0.72\n",
            "Epoch [29/30] - Train Loss: 0.3583, Validation Loss: 0.8874\n",
            "\n",
            "Pearson correlation: 0.74\n",
            "Spearman correlation: 0.73\n",
            "Epoch [30/30] - Train Loss: 0.3506, Validation Loss: 0.8851\n"
          ]
        }
      ],
      "source": [
        "class Warping(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert_config,\n",
        "                 bert_path: str):\n",
        "        \"\"\"\n",
        "        Initialize a BERT-based classifier model.\n",
        "\n",
        "        Args:\n",
        "            bert_config: BERT model configuration.\n",
        "            bert_path (str): Path to a pre-trained BERT model checkpoint.\n",
        "\n",
        "        Attributes:\n",
        "            bert (Bert): Pre-trained BERT model.\n",
        "\n",
        "        \"\"\"\n",
        "        super(Warping, self).__init__()\n",
        "        self.bert = Bert(bert_config).load_model(bert_path)\n",
        "        self.temperature = nn.Parameter(torch.tensor([0.01]))\n",
        "        self.projection = nn.Linear(128*4,256)\n",
        "        self.mu = 0.3\n",
        "        self.std = 0.2\n",
        "        self.n_warps = 100\n",
        "\n",
        "    def forward(self,\n",
        "                sentence_1_ids: torch.Tensor,\n",
        "                sentence_1_mask: torch.Tensor,\n",
        "                sentence_2_ids: torch.Tensor,\n",
        "                sentence_2_mask: torch.Tensor,\n",
        "                labels: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the BERT-based classifier.\n",
        "\n",
        "        Args:\n",
        "            sentence_1_ids (torch.Tensor): Input tensor for sentence 1 token IDs.\n",
        "            sentence_1_mask (torch.Tensor): Input tensor for sentence 1 attention mask.\n",
        "            sentence_2_ids (torch.Tensor): Input tensor for sentence 2 token IDs.\n",
        "            sentence_2_mask (torch.Tensor): Input tensor for sentence 2 attention mask.\n",
        "            labels (torch.Tensor): Ground truth class labels.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Loss value.\n",
        "\n",
        "        \"\"\"\n",
        "        embeddings_1 = self.bert(input_ids=sentence_1_ids, attention_mask=sentence_1_mask, training=True)[1]\n",
        "        embeddings_2 = self.bert(input_ids=sentence_2_ids, attention_mask=sentence_2_mask, training=True)[1]\n",
        "        return self.loss(embeddings_1, embeddings_2)\n",
        "\n",
        "    def warp(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
        "        shape = embeddings.shape\n",
        "        scales = torch.normal(mean=self.mu, std=self.std, size=shape) + torch.ones(shape)\n",
        "        embeddings = torch.mul(embeddings, scales.to(embeddings.device))\n",
        "        return embeddings\n",
        "\n",
        "    def loss(self, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "        a = self.warp(F.normalize(a, p=2, dim=1))\n",
        "        b = self.warp(F.normalize(b, p=2, dim=1))\n",
        "        c = torch.cat((a,b,torch.abs(a-b)), dim=-1)\n",
        "        similarity_matrix = (a @ b.T) / self.temperature \n",
        "        labels = torch.arange(similarity_matrix.shape[0]).to(similarity_matrix.device)\n",
        "        loss_a = F.cross_entropy(similarity_matrix, labels)\n",
        "        loss_b = F.cross_entropy(similarity_matrix.T, labels)\n",
        "        return (loss_a + loss_b)/2 \n",
        "    \n",
        "model = train_and_evaluate(Warping, tokenizer, sts_testloader, num_epochs=30, batch_size=256, maxlen=maxlen)\n",
        "torch.save(model.bert.state_dict(), 'weights/warping_bert.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 3622\n",
            "Validation: 319\n",
            "4517249\n",
            "\n",
            "Pearson correlation: 0.50\n",
            "Spearman correlation: 0.49\n",
            "Epoch [1/30] - Train Loss: 4.4451, Validation Loss: 3.4558\n",
            "\n",
            "Pearson correlation: 0.56\n",
            "Spearman correlation: 0.54\n",
            "Epoch [2/30] - Train Loss: 3.8975, Validation Loss: 3.1812\n",
            "\n",
            "Pearson correlation: 0.58\n",
            "Spearman correlation: 0.56\n",
            "Epoch [3/30] - Train Loss: 3.5175, Validation Loss: 3.2296\n",
            "\n",
            "Pearson correlation: 0.61\n",
            "Spearman correlation: 0.58\n",
            "Epoch [4/30] - Train Loss: 3.2363, Validation Loss: 2.8587\n",
            "\n",
            "Pearson correlation: 0.62\n",
            "Spearman correlation: 0.59\n",
            "Epoch [5/30] - Train Loss: 3.0322, Validation Loss: 2.7317\n",
            "\n",
            "Pearson correlation: 0.62\n",
            "Spearman correlation: 0.60\n",
            "Epoch [6/30] - Train Loss: 2.8170, Validation Loss: 2.7975\n",
            "\n",
            "Pearson correlation: 0.63\n",
            "Spearman correlation: 0.61\n",
            "Epoch [7/30] - Train Loss: 2.6958, Validation Loss: 2.6720\n",
            "\n",
            "Pearson correlation: 0.63\n",
            "Spearman correlation: 0.61\n",
            "Epoch [8/30] - Train Loss: 2.5662, Validation Loss: 2.6377\n",
            "\n",
            "Pearson correlation: 0.64\n",
            "Spearman correlation: 0.61\n",
            "Epoch [9/30] - Train Loss: 2.4464, Validation Loss: 2.5854\n",
            "\n",
            "Pearson correlation: 0.64\n",
            "Spearman correlation: 0.62\n",
            "Epoch [10/30] - Train Loss: 2.3295, Validation Loss: 2.5783\n",
            "\n",
            "Pearson correlation: 0.64\n",
            "Spearman correlation: 0.62\n",
            "Epoch [11/30] - Train Loss: 2.2621, Validation Loss: 2.3953\n",
            "\n",
            "Pearson correlation: 0.64\n",
            "Spearman correlation: 0.62\n",
            "Epoch [12/30] - Train Loss: 2.1693, Validation Loss: 2.4471\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.63\n",
            "Epoch [13/30] - Train Loss: 2.1119, Validation Loss: 2.3354\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.63\n",
            "Epoch [14/30] - Train Loss: 2.0348, Validation Loss: 2.3903\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.63\n",
            "Epoch [15/30] - Train Loss: 1.9512, Validation Loss: 2.3525\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.63\n",
            "Epoch [16/30] - Train Loss: 1.8879, Validation Loss: 2.2255\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.63\n",
            "Epoch [17/30] - Train Loss: 1.8385, Validation Loss: 2.4335\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.63\n",
            "Epoch [18/30] - Train Loss: 1.7586, Validation Loss: 2.4741\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.63\n",
            "Epoch [19/30] - Train Loss: 1.7431, Validation Loss: 2.3100\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [20/30] - Train Loss: 1.6906, Validation Loss: 2.3741\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [21/30] - Train Loss: 1.6486, Validation Loss: 2.4039\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [22/30] - Train Loss: 1.5813, Validation Loss: 2.3646\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [23/30] - Train Loss: 1.5361, Validation Loss: 2.5204\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [24/30] - Train Loss: 1.5056, Validation Loss: 2.2408\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [25/30] - Train Loss: 1.4947, Validation Loss: 2.2939\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [26/30] - Train Loss: 1.4209, Validation Loss: 2.3562\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [27/30] - Train Loss: 1.3720, Validation Loss: 2.1357\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [28/30] - Train Loss: 1.3740, Validation Loss: 2.3702\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [29/30] - Train Loss: 1.3276, Validation Loss: 2.3666\n",
            "\n",
            "Pearson correlation: 0.67\n",
            "Spearman correlation: 0.64\n",
            "Epoch [30/30] - Train Loss: 1.2972, Validation Loss: 2.4669\n"
          ]
        }
      ],
      "source": [
        "class Warping(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert_config,\n",
        "                 bert_path: str):\n",
        "        \"\"\"\n",
        "        Initialize a BERT-based classifier model.\n",
        "\n",
        "        Args:\n",
        "            bert_config: BERT model configuration.\n",
        "            bert_path (str): Path to a pre-trained BERT model checkpoint.\n",
        "\n",
        "        Attributes:\n",
        "            bert (Bert): Pre-trained BERT model.\n",
        "\n",
        "        \"\"\"\n",
        "        super(Warping, self).__init__()\n",
        "        self.bert = Bert(bert_config).load_model(bert_path)\n",
        "        self.temperature = nn.Parameter(torch.tensor([0.01]))\n",
        "        self.projection = nn.Linear(128*4,256)\n",
        "        self.mu = 0.3\n",
        "        self.std = 0.2\n",
        "        self.n_warps = 100\n",
        "\n",
        "    def forward(self,\n",
        "                sentence_1_ids: torch.Tensor,\n",
        "                sentence_1_mask: torch.Tensor,\n",
        "                sentence_2_ids: torch.Tensor,\n",
        "                sentence_2_mask: torch.Tensor,\n",
        "                labels: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the BERT-based classifier.\n",
        "\n",
        "        Args:\n",
        "            sentence_1_ids (torch.Tensor): Input tensor for sentence 1 token IDs.\n",
        "            sentence_1_mask (torch.Tensor): Input tensor for sentence 1 attention mask.\n",
        "            sentence_2_ids (torch.Tensor): Input tensor for sentence 2 token IDs.\n",
        "            sentence_2_mask (torch.Tensor): Input tensor for sentence 2 attention mask.\n",
        "            labels (torch.Tensor): Ground truth class labels.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Loss value.\n",
        "\n",
        "        \"\"\"\n",
        "        embeddings_1 = self.bert(input_ids=sentence_1_ids, attention_mask=sentence_1_mask, training=True)[1]\n",
        "        embeddings_2 = self.bert(input_ids=sentence_2_ids, attention_mask=sentence_2_mask, training=True)[1]\n",
        "        return self.loss(embeddings_1, embeddings_2)\n",
        "\n",
        "    def warp(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
        "        scales = torch.normal(mean=0.0, std=0.2, size=embeddings.shape) + 1\n",
        "        embeddings = embeddings * scales.to(embeddings.device)\n",
        "        return F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "    def loss(self, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "        a, b = self.warp(a), self.warp(b)\n",
        "        similarity_matrix = (a @ b.T) / self.temperature \n",
        "        labels = torch.arange(end=a.shape[0], device=a.device)\n",
        "        return F.cross_entropy(similarity_matrix.T, labels)\n",
        "    \n",
        "model = train_and_evaluate(Warping, tokenizer, sts_testloader, num_epochs=30, batch_size=256, maxlen=maxlen)\n",
        "torch.save(model.bert.state_dict(), 'weights/experimental.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A person on a horse jumps over a broken down airplane.\n",
            "A person is outdoors, on a horse.\n",
            "\n",
            "Children smiling and waving at camera\n",
            "There are children present\n",
            "\n",
            "A boy is jumping on skateboard in the middle of a red bridge.\n",
            "The boy does a skateboarding trick.\n",
            "\n",
            "Two blond women are hugging one another.\n",
            "There are women showing affection.\n",
            "\n",
            "A few people in a restaurant setting, one of them is drinking orange juice.\n",
            "The diners are at a restaurant.\n",
            "\n",
            "An older man is drinking orange juice at a restaurant.\n",
            "A man is drinking juice.\n",
            "\n",
            "A man with blond-hair, and a brown shirt drinking out of a public water fountain.\n",
            "A blond man drinking water from a fountain.\n",
            "\n",
            "Two women who just had lunch hugging and saying goodbye.\n",
            "There are two woman in this picture.\n",
            "\n",
            "Two women, holding food carryout containers, hug.\n",
            "Two women hug each other.\n",
            "\n",
            "A Little League team tries to catch a runner sliding into a base in an afternoon game.\n",
            "A team is trying to tag a runner out.\n",
            "\n",
            "Train: 3622\n",
            "Validation: 319\n",
            "4517249\n",
            "\n",
            "Pearson correlation: 0.50\n",
            "Spearman correlation: 0.49\n",
            "Epoch [1/30] - Train Loss: 4.2697, Validation Loss: 3.3130\n",
            "\n",
            "Pearson correlation: 0.56\n",
            "Spearman correlation: 0.54\n",
            "Epoch [2/30] - Train Loss: 3.6727, Validation Loss: 2.9501\n",
            "\n",
            "Pearson correlation: 0.58\n",
            "Spearman correlation: 0.56\n",
            "Epoch [3/30] - Train Loss: 3.3532, Validation Loss: 2.8992\n",
            "\n",
            "Pearson correlation: 0.60\n",
            "Spearman correlation: 0.58\n",
            "Epoch [4/30] - Train Loss: 3.0319, Validation Loss: 2.7578\n",
            "\n",
            "Pearson correlation: 0.61\n",
            "Spearman correlation: 0.59\n",
            "Epoch [5/30] - Train Loss: 2.8230, Validation Loss: 2.5339\n",
            "\n",
            "Pearson correlation: 0.62\n",
            "Spearman correlation: 0.60\n",
            "Epoch [6/30] - Train Loss: 2.6468, Validation Loss: 2.4805\n",
            "\n",
            "Pearson correlation: 0.63\n",
            "Spearman correlation: 0.61\n",
            "Epoch [7/30] - Train Loss: 2.5374, Validation Loss: 2.5271\n",
            "\n",
            "Pearson correlation: 0.64\n",
            "Spearman correlation: 0.62\n",
            "Epoch [8/30] - Train Loss: 2.3699, Validation Loss: 2.2589\n",
            "\n",
            "Pearson correlation: 0.64\n",
            "Spearman correlation: 0.62\n",
            "Epoch [9/30] - Train Loss: 2.2886, Validation Loss: 2.4373\n",
            "\n",
            "Pearson correlation: 0.64\n",
            "Spearman correlation: 0.63\n",
            "Epoch [10/30] - Train Loss: 2.1776, Validation Loss: 2.3004\n",
            "\n",
            "Pearson correlation: 0.64\n",
            "Spearman correlation: 0.63\n",
            "Epoch [11/30] - Train Loss: 2.0879, Validation Loss: 2.3354\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.63\n",
            "Epoch [12/30] - Train Loss: 2.0031, Validation Loss: 2.2628\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.63\n",
            "Epoch [13/30] - Train Loss: 1.9315, Validation Loss: 2.3088\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.63\n",
            "Epoch [14/30] - Train Loss: 1.8825, Validation Loss: 2.1874\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.63\n",
            "Epoch [15/30] - Train Loss: 1.8204, Validation Loss: 2.1709\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.64\n",
            "Epoch [16/30] - Train Loss: 1.7740, Validation Loss: 2.1832\n",
            "\n",
            "Pearson correlation: 0.65\n",
            "Spearman correlation: 0.64\n",
            "Epoch [17/30] - Train Loss: 1.6985, Validation Loss: 2.3401\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [18/30] - Train Loss: 1.6292, Validation Loss: 2.2711\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [19/30] - Train Loss: 1.6124, Validation Loss: 2.1274\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [20/30] - Train Loss: 1.5512, Validation Loss: 2.0857\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [21/30] - Train Loss: 1.5203, Validation Loss: 2.3177\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [22/30] - Train Loss: 1.4589, Validation Loss: 2.2340\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [23/30] - Train Loss: 1.4250, Validation Loss: 2.1958\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.64\n",
            "Epoch [24/30] - Train Loss: 1.3791, Validation Loss: 2.3466\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.65\n",
            "Epoch [25/30] - Train Loss: 1.3717, Validation Loss: 2.3398\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.65\n",
            "Epoch [26/30] - Train Loss: 1.3049, Validation Loss: 2.2860\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.65\n",
            "Epoch [27/30] - Train Loss: 1.2679, Validation Loss: 2.3895\n",
            "\n",
            "Pearson correlation: 0.66\n",
            "Spearman correlation: 0.65\n",
            "Epoch [28/30] - Train Loss: 1.2747, Validation Loss: 2.0932\n",
            "\n",
            "Pearson correlation: 0.67\n",
            "Spearman correlation: 0.65\n",
            "Epoch [29/30] - Train Loss: 1.1671, Validation Loss: 2.4345\n",
            "\n",
            "Pearson correlation: 0.67\n",
            "Spearman correlation: 0.65\n",
            "Epoch [30/30] - Train Loss: 1.1720, Validation Loss: 2.2409\n"
          ]
        }
      ],
      "source": [
        "class Warping(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert_config,\n",
        "                 bert_path: str):\n",
        "        \"\"\"\n",
        "        Initialize a BERT-based classifier model.\n",
        "\n",
        "        Args:\n",
        "            bert_config: BERT model configuration.\n",
        "            bert_path (str): Path to a pre-trained BERT model checkpoint.\n",
        "\n",
        "        Attributes:\n",
        "            bert (Bert): Pre-trained BERT model.\n",
        "\n",
        "        \"\"\"\n",
        "        super(Warping, self).__init__()\n",
        "        self.bert = Bert(bert_config).load_model(bert_path)\n",
        "        self.temperature = nn.Parameter(torch.tensor([0.01]))\n",
        "        self.projection = nn.Linear(128*4,256)\n",
        "        self.mu = 0.3\n",
        "        self.std = 0.2\n",
        "        self.n_warps = 100\n",
        "\n",
        "    def forward(self,\n",
        "                sentence_1_ids: torch.Tensor,\n",
        "                sentence_1_mask: torch.Tensor,\n",
        "                sentence_2_ids: torch.Tensor,\n",
        "                sentence_2_mask: torch.Tensor,\n",
        "                labels: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the BERT-based classifier.\n",
        "\n",
        "        Args:\n",
        "            sentence_1_ids (torch.Tensor): Input tensor for sentence 1 token IDs.\n",
        "            sentence_1_mask (torch.Tensor): Input tensor for sentence 1 attention mask.\n",
        "            sentence_2_ids (torch.Tensor): Input tensor for sentence 2 token IDs.\n",
        "            sentence_2_mask (torch.Tensor): Input tensor for sentence 2 attention mask.\n",
        "            labels (torch.Tensor): Ground truth class labels.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Loss value.\n",
        "\n",
        "        \"\"\"\n",
        "        embeddings_1 = self.bert(input_ids=sentence_1_ids, attention_mask=sentence_1_mask, training=True)[1]\n",
        "        embeddings_2 = self.bert(input_ids=sentence_2_ids, attention_mask=sentence_2_mask, training=True)[1]\n",
        "        return self.loss(embeddings_1, embeddings_2)\n",
        "\n",
        "    def loss(self, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "        a = F.normalize(a, p=2, dim=1)\n",
        "        b = F.normalize(b, p=2, dim=1)\n",
        "        similarity_matrix = (b @ a.T) / self.temperature \n",
        "        labels = torch.arange(end=a.shape[0], device=a.device) \n",
        "        return F.cross_entropy(similarity_matrix, labels)\n",
        "    \n",
        "model = train_and_evaluate(Warping, tokenizer, sts_testloader, num_epochs=30, batch_size=256, maxlen=maxlen)\n",
        "torch.save(model.bert.state_dict(), 'weights/experimental.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 50.9167,  -2.1169,  39.5748, -24.6775,  88.2285],\n",
            "        [  7.5358, -24.0080,  15.5092,  74.0901,  35.6088],\n",
            "        [-12.0127,  26.0580,   2.0394,  -9.9870, -61.7342],\n",
            "        [ 30.5556,  18.5826,  44.6844, -42.4859,  33.2288],\n",
            "        [  1.2581,  50.2989,  18.0844,   9.9399, -19.9169]])\n",
            "tensor([0, 1, 2, 3, 4])\n",
            "tensor([[ 50.9167,   7.5358, -12.0127,  30.5556,   1.2581],\n",
            "        [ -2.1169, -24.0080,  26.0580,  18.5826,  50.2989],\n",
            "        [ 39.5748,  15.5092,   2.0394,  44.6844,  18.0844],\n",
            "        [-24.6775,  74.0901,  -9.9870, -42.4859,   9.9399],\n",
            "        [ 88.2285,  35.6088, -61.7342,  33.2288, -19.9169]])\n",
            "Loss value: 68.3358383178711\n"
          ]
        }
      ],
      "source": [
        "def loss(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    a = F.normalize(a, p=2, dim=1)\n",
        "    b = F.normalize(b, p=2, dim=1)\n",
        "    similarity_matrix = (a @ b.T) / 0.01\n",
        "    \n",
        "    print(similarity_matrix)\n",
        "    labels = torch.arange(similarity_matrix.shape[0])\n",
        "    print(labels)\n",
        "    print(similarity_matrix.T)\n",
        "    return F.cross_entropy(similarity_matrix.T, labels)\n",
        "\n",
        "a = torch.randn(5, 10)\n",
        "b = torch.randn(5, 10)\n",
        "loss_value = loss(a, b)\n",
        "print(\"Loss value:\", loss_value.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 50.9167,   7.5358, -12.0127,  30.5556,   1.2581],\n",
            "        [ -2.1169, -24.0080,  26.0580,  18.5826,  50.2989],\n",
            "        [ 39.5748,  15.5092,   2.0394,  44.6844,  18.0844],\n",
            "        [-24.6775,  74.0901,  -9.9870, -42.4859,   9.9399],\n",
            "        [ 88.2285,  35.6088, -61.7342,  33.2288, -19.9169]])\n",
            "tensor([0, 1, 2, 3, 4])\n",
            "tensor([[ 50.9167,  -2.1169,  39.5748, -24.6775,  88.2285],\n",
            "        [  7.5358, -24.0080,  15.5092,  74.0901,  35.6088],\n",
            "        [-12.0127,  26.0580,   2.0394,  -9.9870, -61.7342],\n",
            "        [ 30.5556,  18.5826,  44.6844, -42.4859,  33.2288],\n",
            "        [  1.2581,  50.2989,  18.0844,   9.9399, -19.9169]])\n"
          ]
        }
      ],
      "source": [
        "loss_value = loss(b, a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[100.0000, -22.5342,  68.4319,  29.2120, -45.7219, 100.0000, -22.5342,\n",
            "          68.4319,  29.2120, -45.7219],\n",
            "        [-22.5342, 100.0000,  47.3745, -34.0043,  59.5988, -22.5342, 100.0000,\n",
            "          47.3745, -34.0043,  59.5988],\n",
            "        [ 68.4319,  47.3745, 100.0000,  -7.5773,   7.4423,  68.4319,  47.3745,\n",
            "         100.0000,  -7.5773,   7.4423],\n",
            "        [ 29.2120, -34.0043,  -7.5773, 100.0000, -24.1734,  29.2120, -34.0043,\n",
            "          -7.5773, 100.0000, -24.1734],\n",
            "        [-45.7219,  59.5988,   7.4423, -24.1734, 100.0000, -45.7219,  59.5988,\n",
            "           7.4423, -24.1734, 100.0000],\n",
            "        [100.0000, -22.5342,  68.4319,  29.2120, -45.7219, 100.0000, -22.5342,\n",
            "          68.4319,  29.2120, -45.7219],\n",
            "        [-22.5342, 100.0000,  47.3745, -34.0043,  59.5988, -22.5342, 100.0000,\n",
            "          47.3745, -34.0043,  59.5988],\n",
            "        [ 68.4319,  47.3745, 100.0000,  -7.5773,   7.4423,  68.4319,  47.3745,\n",
            "         100.0000,  -7.5773,   7.4423],\n",
            "        [ 29.2120, -34.0043,  -7.5773, 100.0000, -24.1734,  29.2120, -34.0043,\n",
            "          -7.5773, 100.0000, -24.1734],\n",
            "        [-45.7219,  59.5988,   7.4423, -24.1734, 100.0000, -45.7219,  59.5988,\n",
            "           7.4423, -24.1734, 100.0000]])\n",
            "tensor([[-22.5342,  68.4319,  29.2120, -45.7219, 100.0000, -22.5342,  68.4319,\n",
            "          29.2120, -45.7219],\n",
            "        [-22.5342,  47.3745, -34.0043,  59.5988, -22.5342, 100.0000,  47.3745,\n",
            "         -34.0043,  59.5988],\n",
            "        [ 68.4319,  47.3745,  -7.5773,   7.4423,  68.4319,  47.3745, 100.0000,\n",
            "          -7.5773,   7.4423],\n",
            "        [ 29.2120, -34.0043,  -7.5773, -24.1734,  29.2120, -34.0043,  -7.5773,\n",
            "         100.0000, -24.1734],\n",
            "        [-45.7219,  59.5988,   7.4423, -24.1734, -45.7219,  59.5988,   7.4423,\n",
            "         -24.1734, 100.0000],\n",
            "        [100.0000, -22.5342,  68.4319,  29.2120, -45.7219, -22.5342,  68.4319,\n",
            "          29.2120, -45.7219],\n",
            "        [-22.5342, 100.0000,  47.3745, -34.0043,  59.5988, -22.5342,  47.3745,\n",
            "         -34.0043,  59.5988],\n",
            "        [ 68.4319,  47.3745, 100.0000,  -7.5773,   7.4423,  68.4319,  47.3745,\n",
            "          -7.5773,   7.4423],\n",
            "        [ 29.2120, -34.0043,  -7.5773, 100.0000, -24.1734,  29.2120, -34.0043,\n",
            "          -7.5773, -24.1734],\n",
            "        [-45.7219,  59.5988,   7.4423, -24.1734, 100.0000, -45.7219,  59.5988,\n",
            "           7.4423, -24.1734]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(55.2632)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def loss2(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
        "    features = torch.cat([a, b], dim=0)\n",
        "    normalized_features = F.normalize(features, p=2, dim=1)\n",
        "    similarity_matrix = torch.mm(normalized_features, normalized_features.T)\n",
        "    logits = similarity_matrix / 0.01\n",
        "    print(logits)\n",
        "    batch_size = a.size(0)\n",
        "    mask = torch.eye(batch_size, dtype=torch.bool, device=a.device)\n",
        "    labels = torch.cat([torch.arange(batch_size, device=a.device), torch.arange(batch_size, device=a.device)])\n",
        "    logits = logits.masked_select(~torch.block_diag(mask, mask)).view(2 * batch_size, -1)\n",
        "    print(logits)\n",
        "    loss = F.cross_entropy(logits, labels)\n",
        "    return loss\n",
        "loss2(a,a)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LPBlKHoVTp1v",
        "s8dEdZoc815Y"
      ],
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
